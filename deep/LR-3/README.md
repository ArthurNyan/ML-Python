# Лабораторная работа 3: Нейронная сеть с обратным распространением ошибки

## Описание
Реализация полносвязной нейронной сети с нуля на Python для задачи бинарной классификации. 
Включает в себя:
- Реализацию алгоритма обратного распространения ошибки
- Градиентный спуск на мини-батчах
- Различные функции активации (ReLU, Tanh, Sigmoid)
- Визуализацию результатов

## Структура проекта
- `neural_network.py` - основная реализация нейронной сети
- `test_network.py` - пример использования сети на датасете make_moons
- `requirements.txt` - зависимости проекта

## Установка
1. Создайте виртуальное окружение:
```bash
python -m venv venv
source venv/bin/activate  # для Linux/Mac
# или
venv\Scripts\activate  # для Windows
```

2. Установите зависимости:
```bash
pip install -r requirements.txt
```

## Использование
Запустите тестовый скрипт:
```bash
python test_network.py
```

Это создаст модель нейронной сети, обучит её на датасете make_moons и выведет:
- Точность на обучающей и тестовой выборках
- Визуализацию границы решений
- График функции потерь в процессе обучения

## Параметры сети
В текущей конфигурации используется архитектура:
- Входной слой: 2 нейрона
- Первый скрытый слой: 10 нейронов (ReLU)
- Второй скрытый слой: 5 нейронов (ReLU)
- Выходной слой: 1 нейрон (Sigmoid)

Параметры обучения:
- Скорость обучения: 0.01
- Размер мини-батча: 32
- Количество эпох: 1000

## Дополнительно
Вы можете экспериментировать с:
- Архитектурой сети (изменяя `layer_dims`)
- Функциями активации (выбирая между 'relu' и 'tanh')
- Гиперпараметрами обучения
- Различными датасетами 